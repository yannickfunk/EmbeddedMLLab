{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UM2oJy-ag5H3",
    "outputId": "de66a4cb-5fe8-4e57-c220-fcad944c9c27"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = False\n",
    "DATA_PATH = \"data/\"\n",
    "\n",
    "# if in colab, make colab setup\n",
    "if 'google.colab' in sys.modules:\n",
    "    \n",
    "    IN_COLAB = True\n",
    "    SSH_DIR = '/root/.ssh'\n",
    "    ZIP_PATH = \"/content/drive/MyDrive/embedded_ml_data/VOCdevkit.zip\"\n",
    "    FILENAME = \"dev-notebook.ipynb\"\n",
    "    BRANCH = \"michael\"\n",
    "\n",
    "    # Setup ssh-auth to github\n",
    "    try:\n",
    "        os.makedirs(SSH_DIR)\n",
    "    except FileExistsError:\n",
    "        # directory already exists\n",
    "        pass\n",
    "\n",
    "    !ssh-keyscan github.com >> /root/.ssh/known_hosts\n",
    "    !echo 'PUBKEY' > /root/.ssh/id_rsa.pub\n",
    "    !echo -e \"PRIVKEY\" > /root/.ssh/id_rsa\n",
    "    !chmod 644 /root/.ssh/known_hosts\n",
    "    !chmod 600 /root/.ssh/id_rsa\n",
    "    !ssh -T git@github.com\n",
    "\n",
    "    # Setup working environment\n",
    "    !rm -rf models\n",
    "    !rm -rf utils\n",
    "    !git clone -b $BRANCH https://github.com/yannickfunk/EmbeddedMLLab tmp\n",
    "    !rm tmp/$FILENAME\n",
    "    !mv tmp/* .\n",
    "    !rm -rf tmp\n",
    "    !rm -rf sample_data\n",
    "    %pip install -r requirements.txt\n",
    "\n",
    "    # Setup data\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    try:\n",
    "      os.makedirs('./data')\n",
    "    except FileExistsError:\n",
    "      # directory already exists\n",
    "      pass\n",
    "    !cp  $ZIP_PATH data/\n",
    "    %pushd data\n",
    "    !unzip -qq VOCdevkit.zip\n",
    "    %popd\n",
    "    drive.flush_and_unmount()\n",
    "\n",
    "    \n",
    "try:\n",
    "    os.makedirs('./checkpoints')\n",
    "    os.makedirs('./checkpoints/results')\n",
    "except FileExistsError:\n",
    "    # directory already exists\n",
    "    pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix recursion error when using checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Tensorboard logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logger\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "LOGDIR='logs'\n",
    "TENSORBOARD_DIR=LOGDIR+'/lightning_logs'\n",
    "\n",
    "tensorboard = pl.loggers.TensorBoardLogger(save_dir=LOGDIR, default_hp_metric=True, log_graph=True)\n",
    "\n",
    "try:\n",
    "    os.makedirs('./'+LOGDIR+'/lightning_logs')\n",
    "except FileExistsError:\n",
    "    # directory already exists\n",
    "    pass\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-115bd9e545da8951\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-115bd9e545da8951\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir $TENSORBOARD_DIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from models.tinyyolov2 import TinyYoloV2PersonOnly\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "from utils.dataloader import VOCDataModule\n",
    "\n",
    "from pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import nni\n",
    "from nni.compression.pytorch import LightningEvaluator\n",
    "\n",
    "\n",
    "# Setting up callbacks\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode='min', verbose=True)\n",
    "checkpointing = ModelCheckpoint(\n",
    "    save_top_k=3,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    verbose=True,\n",
    "    auto_insert_metric_name=True,\n",
    "    save_last=True\n",
    ")\n",
    "\n",
    "trainer = nni.trace(pl.Trainer)(\n",
    "    max_epochs=50,\n",
    "    auto_scale_batch_size='binsearch',\n",
    "    accelerator=\"auto\",\n",
    "    devices=[0] if torch.cuda.is_available() else None,\n",
    "    accumulate_grad_batches=1,\n",
    "    logger=tensorboard,\n",
    "    log_every_n_steps=1,\n",
    "    fast_dev_run= True if not torch.cuda.is_available() else False,\n",
    "    callbacks=[\n",
    "    lr_monitor,\n",
    "    early_stopping,\n",
    "    checkpointing\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "model = TinyYoloV2PersonOnly(learning_rate=0.01)\n",
    "model.load_pt_from_disk(DATA_PATH + \"/voc_pretrained.pt\", discard_layer_8=False)\n",
    "\n",
    "data = nni.trace(VOCDataModule)(person_only=True)\n",
    "\n",
    "evaluator = LightningEvaluator(trainer, data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/VOCtrainval_11-May-2012.tar\n",
      "Extracting data/VOCtrainval_11-May-2012.tar to data/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name  | Type            | Params\n",
      "-------------------------------------------\n",
      "0  | loss  | YoloLoss        | 0     \n",
      "1  | pad   | ReflectionPad2d | 0     \n",
      "2  | conv1 | Conv2d          | 432   \n",
      "3  | bn1   | BatchNorm2d     | 32    \n",
      "4  | conv2 | Conv2d          | 4.6 K \n",
      "5  | bn2   | BatchNorm2d     | 64    \n",
      "6  | conv3 | Conv2d          | 18.4 K\n",
      "7  | bn3   | BatchNorm2d     | 128   \n",
      "8  | conv4 | Conv2d          | 73.7 K\n",
      "9  | bn4   | BatchNorm2d     | 256   \n",
      "10 | conv5 | Conv2d          | 294 K \n",
      "11 | bn5   | BatchNorm2d     | 512   \n",
      "12 | conv6 | Conv2d          | 1.2 M \n",
      "13 | bn6   | BatchNorm2d     | 1.0 K \n",
      "14 | conv7 | Conv2d          | 4.7 M \n",
      "15 | bn7   | BatchNorm2d     | 2.0 K \n",
      "16 | conv8 | Conv2d          | 9.4 M \n",
      "17 | bn8   | BatchNorm2d     | 2.0 K \n",
      "18 | conv9 | Conv2d          | 30.8 K\n",
      "-------------------------------------------\n",
      "9.5 M     Trainable params\n",
      "6.3 M     Non-trainable params\n",
      "15.8 M    Total params\n",
      "63.058    Total estimated model params size (MB)\n",
      "/opt/conda/envs/eml/lib/python3.10/site-packages/pytorch_lightning/loggers/tensorboard.py:188: UserWarning: Could not log computational graph to TensorBoard: The `model.example_input_array` attribute is not set or `input_array` was not given.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03979736cf314b19bfe20934c2ea392a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/VOCtrainval_11-May-2012.tar\n",
      "Extracting data/VOCtrainval_11-May-2012.tar to data/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/eml/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541990/work/aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/VOCtrainval_11-May-2012.tar\n",
      "Extracting data/VOCtrainval_11-May-2012.tar to data/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44de5759cd644067bd7c37cfbdaed603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c343549125432eb1e08a2304e14a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 6.638\n",
      "Epoch 0, global step 17: 'val_loss' reached 6.63835 (best 6.63835), saving model to 'logs/lightning_logs/version_21/checkpoints/epoch=0-step=17.ckpt' as top 3\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not pickle object as excessively deep recursion required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/cloudpickle/cloudpickle_fast.py:632\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[39mreturn\u001b[39;00m Pickler\u001b[39m.\u001b[39;49mdump(\u001b[39mself\u001b[39;49m, obj)\n\u001b[1;32m    633\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/cloudpickle/cloudpickle_fast.py:735\u001b[0m, in \u001b[0;36mCloudPickler.reducer_override\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[39mif\u001b[39;00m is_anyclass:\n\u001b[0;32m--> 735\u001b[0m     \u001b[39mreturn\u001b[39;00m _class_reduce(obj)\n\u001b[1;32m    736\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, types\u001b[39m.\u001b[39mFunctionType):\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/cloudpickle/cloudpickle_fast.py:444\u001b[0m, in \u001b[0;36m_class_reduce\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[39mreturn\u001b[39;00m _builtin_type, (_BUILTIN_TYPE_NAMES[obj],)\n\u001b[0;32m--> 444\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m _should_pickle_by_reference(obj):\n\u001b[1;32m    445\u001b[0m     \u001b[39mreturn\u001b[39;00m _dynamic_class_reduce(obj)\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/cloudpickle/cloudpickle.py:256\u001b[0m, in \u001b[0;36m_should_pickle_by_reference\u001b[0;34m(obj, name)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, types\u001b[39m.\u001b[39mFunctionType) \u001b[39mor\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mtype\u001b[39m(obj), \u001b[39mtype\u001b[39m):\n\u001b[0;32m--> 256\u001b[0m     module_and_name \u001b[39m=\u001b[39m _lookup_module_and_qualname(obj, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m    257\u001b[0m     \u001b[39mif\u001b[39;00m module_and_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/cloudpickle/cloudpickle.py:287\u001b[0m, in \u001b[0;36m_lookup_module_and_qualname\u001b[0;34m(obj, name)\u001b[0m\n\u001b[1;32m    285\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m'\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 287\u001b[0m module_name \u001b[39m=\u001b[39m _whichmodule(obj, name)\n\u001b[1;32m    289\u001b[0m \u001b[39mif\u001b[39;00m module_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[39m# In this case, obj.__module__ is None AND obj was not found in any\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     \u001b[39m# imported module. obj is thus treated as dynamic.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/cloudpickle/cloudpickle.py:204\u001b[0m, in \u001b[0;36m_whichmodule\u001b[0;34m(obj, name)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Find the module an object belongs to.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \n\u001b[1;32m    198\u001b[0m \u001b[39mThis function differs from ``pickle.whichmodule`` in two ways:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39m  are considered unwanted side effects.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m \u001b[39mif\u001b[39;00m sys\u001b[39m.\u001b[39;49mversion_info[:\u001b[39m2\u001b[39;49m] \u001b[39m<\u001b[39;49m (\u001b[39m3\u001b[39;49m, \u001b[39m7\u001b[39;49m) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, typing\u001b[39m.\u001b[39mTypeVar):  \u001b[39m# pragma: no branch  # noqa\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     \u001b[39m# Workaround bug in old Python versions: prior to Python 3.7,\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     \u001b[39m# T.__module__ would always be set to \"typing\" even when the TypeVar T\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     \u001b[39m# would be defined in a different module.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mgetattr\u001b[39m(typing, name, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m obj:\n\u001b[1;32m    209\u001b[0m         \u001b[39m# Built-in TypeVar defined in typing such as AnyStr\u001b[39;00m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded in comparison",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluator\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mfit(model,data)\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    607\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    609\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    610\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    644\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    645\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    646\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[0;32m--> 650\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    652\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1112\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1114\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1191\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1190\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1191\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1214\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   1213\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1214\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:295\u001b[0m, in \u001b[0;36mFitLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    294\u001b[0m \u001b[39m# call train epoch end hooks\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_callback_hooks(\u001b[39m\"\u001b[39;49m\u001b[39mon_train_epoch_end\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    296\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_lightning_module_hook(\u001b[39m\"\u001b[39m\u001b[39mon_train_epoch_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    298\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mon_epoch_end()\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1394\u001b[0m, in \u001b[0;36mTrainer._call_callback_hooks\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1392\u001b[0m     \u001b[39mif\u001b[39;00m callable(fn):\n\u001b[1;32m   1393\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Callback]\u001b[39m\u001b[39m{\u001b[39;00mcallback\u001b[39m.\u001b[39mstate_key\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1394\u001b[0m             fn(\u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1396\u001b[0m \u001b[39mif\u001b[39;00m pl_module:\n\u001b[1;32m   1397\u001b[0m     \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1398\u001b[0m     pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:304\u001b[0m, in \u001b[0;36mModelCheckpoint.on_train_epoch_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    302\u001b[0m monitor_candidates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_monitor_candidates(trainer)\n\u001b[1;32m    303\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_every_n_epochs \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m (trainer\u001b[39m.\u001b[39mcurrent_epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_every_n_epochs \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_topk_checkpoint(trainer, monitor_candidates)\n\u001b[1;32m    305\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_last_checkpoint(trainer, monitor_candidates)\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:361\u001b[0m, in \u001b[0;36mModelCheckpoint._save_topk_checkpoint\u001b[0;34m(self, trainer, monitor_candidates)\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[39mraise\u001b[39;00m MisconfigurationException(m)\n\u001b[1;32m    360\u001b[0m         warning_cache\u001b[39m.\u001b[39mwarn(m)\n\u001b[0;32m--> 361\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_monitor_checkpoint(trainer, monitor_candidates)\n\u001b[1;32m    362\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    363\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_none_monitor_checkpoint(trainer, monitor_candidates)\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:659\u001b[0m, in \u001b[0;36mModelCheckpoint._save_monitor_checkpoint\u001b[0;34m(self, trainer, monitor_candidates)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_monitor_top_k(trainer, current):\n\u001b[1;32m    658\u001b[0m     \u001b[39massert\u001b[39;00m current \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 659\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_best_and_save(current, trainer, monitor_candidates)\n\u001b[1;32m    660\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n\u001b[1;32m    661\u001b[0m     epoch \u001b[39m=\u001b[39m monitor_candidates[\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:710\u001b[0m, in \u001b[0;36mModelCheckpoint._update_best_and_save\u001b[0;34m(self, current, trainer, monitor_candidates)\u001b[0m\n\u001b[1;32m    705\u001b[0m     step \u001b[39m=\u001b[39m monitor_candidates[\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    706\u001b[0m     rank_zero_info(\n\u001b[1;32m    707\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39md\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, global step \u001b[39m\u001b[39m{\u001b[39;00mstep\u001b[39m:\u001b[39;00m\u001b[39md\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmonitor\u001b[39m!r}\u001b[39;00m\u001b[39m reached \u001b[39m\u001b[39m{\u001b[39;00mcurrent\u001b[39m:\u001b[39;00m\u001b[39m0.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    708\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m (best \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_model_score\u001b[39m:\u001b[39;00m\u001b[39m0.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m), saving model to \u001b[39m\u001b[39m{\u001b[39;00mfilepath\u001b[39m!r}\u001b[39;00m\u001b[39m as top \u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    709\u001b[0m     )\n\u001b[0;32m--> 710\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_checkpoint(trainer, filepath)\n\u001b[1;32m    712\u001b[0m \u001b[39mif\u001b[39;00m del_filepath \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filepath \u001b[39m!=\u001b[39m del_filepath:\n\u001b[1;32m    713\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remove_checkpoint(trainer, del_filepath)\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:366\u001b[0m, in \u001b[0;36mModelCheckpoint._save_checkpoint\u001b[0;34m(self, trainer, filepath)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_save_checkpoint\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m, filepath: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 366\u001b[0m     trainer\u001b[39m.\u001b[39;49msave_checkpoint(filepath, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_weights_only)\n\u001b[1;32m    368\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_global_step_saved \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mglobal_step\n\u001b[1;32m    370\u001b[0m     \u001b[39m# notify loggers\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1939\u001b[0m, in \u001b[0;36mTrainer.save_checkpoint\u001b[0;34m(self, filepath, weights_only, storage_options)\u001b[0m\n\u001b[1;32m   1934\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1935\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m   1936\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSaving a checkpoint is only possible if a model is attached to the Trainer. Did you call\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1937\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m `Trainer.save_checkpoint()` before calling `Trainer.\u001b[39m\u001b[39m{\u001b[39m\u001b[39mfit,validate,test,predict}`?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1938\u001b[0m     )\n\u001b[0;32m-> 1939\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_checkpoint_connector\u001b[39m.\u001b[39;49msave_checkpoint(filepath, weights_only\u001b[39m=\u001b[39;49mweights_only, storage_options\u001b[39m=\u001b[39;49mstorage_options)\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:511\u001b[0m, in \u001b[0;36mCheckpointConnector.save_checkpoint\u001b[0;34m(self, filepath, weights_only, storage_options)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \n\u001b[1;32m    505\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[39m    storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    510\u001b[0m _checkpoint \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdump_checkpoint(weights_only)\n\u001b[0;32m--> 511\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mstrategy\u001b[39m.\u001b[39;49msave_checkpoint(_checkpoint, filepath, storage_options\u001b[39m=\u001b[39;49mstorage_options)\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:466\u001b[0m, in \u001b[0;36mStrategy.save_checkpoint\u001b[0;34m(self, checkpoint, filepath, storage_options)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \n\u001b[1;32m    460\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39m    storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_global_zero:\n\u001b[0;32m--> 466\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheckpoint_io\u001b[39m.\u001b[39;49msave_checkpoint(checkpoint, filepath, storage_options\u001b[39m=\u001b[39;49mstorage_options)\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/lightning_fabric/plugins/io/torch_io.py:54\u001b[0m, in \u001b[0;36mTorchCheckpointIO.save_checkpoint\u001b[0;34m(self, checkpoint, path, storage_options)\u001b[0m\n\u001b[1;32m     51\u001b[0m fs\u001b[39m.\u001b[39mmakedirs(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(path), exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39m# write the checkpoint dictionary on the file\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     _atomic_save(checkpoint, path)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m     56\u001b[0m     \u001b[39m# todo: is this try catch necessary still?\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[39m# https://github.com/Lightning-AI/lightning/pull/431\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[39m# TODO(fabric): Fabric doesn't support hyperparameters in the checkpoint, so this should be refactored\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhyper_parameters\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/lightning_fabric/utilities/cloud_io.py:67\u001b[0m, in \u001b[0;36m_atomic_save\u001b[0;34m(checkpoint, filepath)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Saves a checkpoint atomically, avoiding the creation of incomplete checkpoints.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39m        This points to the file that the checkpoint will be stored in.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m bytesbuffer \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO()\n\u001b[0;32m---> 67\u001b[0m torch\u001b[39m.\u001b[39;49msave(checkpoint, bytesbuffer)\n\u001b[1;32m     68\u001b[0m \u001b[39mwith\u001b[39;00m fsspec\u001b[39m.\u001b[39mopen(filepath, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     69\u001b[0m     f\u001b[39m.\u001b[39mwrite(bytesbuffer\u001b[39m.\u001b[39mgetvalue())\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/torch/serialization.py:423\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    422\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 423\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    424\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/torch/serialization.py:635\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    633\u001b[0m pickler \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mPickler(data_buf, protocol\u001b[39m=\u001b[39mpickle_protocol)\n\u001b[1;32m    634\u001b[0m pickler\u001b[39m.\u001b[39mpersistent_id \u001b[39m=\u001b[39m persistent_id\n\u001b[0;32m--> 635\u001b[0m pickler\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m    636\u001b[0m data_value \u001b[39m=\u001b[39m data_buf\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    637\u001b[0m zip_file\u001b[39m.\u001b[39mwrite_record(\u001b[39m'\u001b[39m\u001b[39mdata.pkl\u001b[39m\u001b[39m'\u001b[39m, data_value, \u001b[39mlen\u001b[39m(data_value))\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/nni/common/serializer.py:531\u001b[0m, in \u001b[0;36m_trace_cls.<locals>.wrapper.__reduce__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__reduce__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    520\u001b[0m     \u001b[39m# The issue that decorator and pickler doesn't play well together is well known.\u001b[39;00m\n\u001b[1;32m    521\u001b[0m     \u001b[39m# The workaround solution is to use a fool class (_pickling_object) which pretends to be the pickled object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    529\u001b[0m \n\u001b[1;32m    530\u001b[0m     \u001b[39m# Store the inner class. The wrapped class couldn't be properly pickled.\u001b[39;00m\n\u001b[0;32m--> 531\u001b[0m     type_ \u001b[39m=\u001b[39m cloudpickle\u001b[39m.\u001b[39;49mdumps(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m__wrapped__)\n\u001b[1;32m    533\u001b[0m     \u001b[39m# in case they have customized ``__getstate__``.\u001b[39;00m\n\u001b[1;32m    534\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m__getstate__\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/cloudpickle/cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mwith\u001b[39;00m io\u001b[39m.\u001b[39mBytesIO() \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     70\u001b[0m     cp \u001b[39m=\u001b[39m CloudPickler(\n\u001b[1;32m     71\u001b[0m         file, protocol\u001b[39m=\u001b[39mprotocol, buffer_callback\u001b[39m=\u001b[39mbuffer_callback\n\u001b[1;32m     72\u001b[0m     )\n\u001b[0;32m---> 73\u001b[0m     cp\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m file\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/cloudpickle/cloudpickle_fast.py:639\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrecursion\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]:\n\u001b[1;32m    635\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    636\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not pickle object as excessively deep recursion \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    637\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrequired.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    638\u001b[0m     )\n\u001b[0;32m--> 639\u001b[0m     \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mPicklingError(msg) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not pickle object as excessively deep recursion required."
     ]
    }
   ],
   "source": [
    "evaluator.trainer.fit(model,data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/VOCtrainval_11-May-2012.tar\n",
      "Extracting data/VOCtrainval_11-May-2012.tar to data/\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/home/funnymonkey/eml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m test_precision \u001b[39m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m test_recall \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 14\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mload_from_checkpoint(checkpoint_path\u001b[39m=\u001b[39;49mcheckpointing\u001b[39m.\u001b[39;49mbest_model_path)\n\u001b[1;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m inputs, targets \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(test_loader, total\u001b[39m=\u001b[39m\u001b[39m350\u001b[39m):\n\u001b[1;32m     17\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:139\u001b[0m, in \u001b[0;36mModelIO.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_from_checkpoint\u001b[39m(\n\u001b[1;32m     61\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m     67\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Self:  \u001b[39m# type: ignore[valid-type]\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_from_checkpoint(\n\u001b[1;32m    140\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[1;32m    141\u001b[0m         checkpoint_path,\n\u001b[1;32m    142\u001b[0m         map_location,\n\u001b[1;32m    143\u001b[0m         hparams_file,\n\u001b[1;32m    144\u001b[0m         strict,\n\u001b[1;32m    145\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    146\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:160\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m     map_location \u001b[39m=\u001b[39m cast(_MAP_LOCATION_TYPE, \u001b[39mlambda\u001b[39;00m storage, loc: storage)\n\u001b[1;32m    159\u001b[0m \u001b[39mwith\u001b[39;00m pl_legacy_patch():\n\u001b[0;32m--> 160\u001b[0m     checkpoint \u001b[39m=\u001b[39m pl_load(checkpoint_path, map_location\u001b[39m=\u001b[39;49mmap_location)\n\u001b[1;32m    162\u001b[0m \u001b[39m# convert legacy checkpoints to the new format\u001b[39;00m\n\u001b[1;32m    163\u001b[0m checkpoint \u001b[39m=\u001b[39m _pl_migrate_checkpoint(\n\u001b[1;32m    164\u001b[0m     checkpoint, checkpoint_path\u001b[39m=\u001b[39m(checkpoint_path \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(checkpoint_path, (\u001b[39mstr\u001b[39m, Path)) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    165\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/lightning_fabric/utilities/cloud_io.py:47\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mhub\u001b[39m.\u001b[39mload_state_dict_from_url(\n\u001b[1;32m     43\u001b[0m         \u001b[39mstr\u001b[39m(path_or_url),\n\u001b[1;32m     44\u001b[0m         map_location\u001b[39m=\u001b[39mmap_location,  \u001b[39m# type: ignore[arg-type] # upstream annotation is not correct\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     )\n\u001b[1;32m     46\u001b[0m fs \u001b[39m=\u001b[39m get_filesystem(path_or_url)\n\u001b[0;32m---> 47\u001b[0m \u001b[39mwith\u001b[39;00m fs\u001b[39m.\u001b[39;49mopen(path_or_url, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mload(f, map_location\u001b[39m=\u001b[39mmap_location)\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/fsspec/spec.py:1106\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m     ac \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mautocommit\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_intrans)\n\u001b[0;32m-> 1106\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(\n\u001b[1;32m   1107\u001b[0m         path,\n\u001b[1;32m   1108\u001b[0m         mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   1109\u001b[0m         block_size\u001b[39m=\u001b[39;49mblock_size,\n\u001b[1;32m   1110\u001b[0m         autocommit\u001b[39m=\u001b[39;49mac,\n\u001b[1;32m   1111\u001b[0m         cache_options\u001b[39m=\u001b[39;49mcache_options,\n\u001b[1;32m   1112\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1113\u001b[0m     )\n\u001b[1;32m   1114\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1115\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39mfsspec\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompression\u001b[39;00m \u001b[39mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/fsspec/implementations/local.py:175\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_mkdir \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmakedirs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent(path), exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 175\u001b[0m \u001b[39mreturn\u001b[39;00m LocalFileOpener(path, mode, fs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/fsspec/implementations/local.py:273\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression \u001b[39m=\u001b[39m get_compression(path, compression)\n\u001b[1;32m    272\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocksize \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[0;32m--> 273\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open()\n",
      "File \u001b[0;32m/opt/conda/envs/eml/lib/python3.10/site-packages/fsspec/implementations/local.py:278\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf\u001b[39m.\u001b[39mclosed:\n\u001b[1;32m    277\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautocommit \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode:\n\u001b[0;32m--> 278\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath, mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode)\n\u001b[1;32m    279\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression:\n\u001b[1;32m    280\u001b[0m             compress \u001b[39m=\u001b[39m compr[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression]\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/home/funnymonkey/eml'"
     ]
    }
   ],
   "source": [
    "from utils.ap import precision_recall_levels, ap, display_roc\n",
    "from utils.yolo import nms, filter_boxes\n",
    "from utils.dataloader import VOCDataLoaderPerson\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "\n",
    "test_loader = VOCDataLoaderPerson(train=False, batch_size=1, data_path=DATA_PATH, n_limit=350)\n",
    "\n",
    "test_precision = []\n",
    "test_recall = []\n",
    "model = model.load_from_checkpoint(checkpoint_path=checkpointing.best_model_path)\n",
    "\n",
    "for inputs, targets in tqdm.tqdm(test_loader, total=350):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs, yolo=True)\n",
    "\n",
    "    #The right threshold values can be adjusted for the target application\n",
    "    outputs = filter_boxes(outputs, 0.0)\n",
    "    outputs = nms(outputs, 0.5)\n",
    "    outputs = torch.tensor(np.array(outputs))\n",
    "\n",
    "    precision, recall = precision_recall_levels(targets[0], outputs[0])\n",
    "    test_precision.append(precision)\n",
    "    test_recall.append(recall)\n",
    "\n",
    "avg_precision = ap(test_precision, test_recall)\n",
    "print(\"average precision: \", avg_precision)\n",
    "display_roc(test_precision, test_recall)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2379398720.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 17\u001b[0;36m\u001b[0m\n\u001b[0;31m    inference_time_unpruned =\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from tasks.pruning import auto_prune\n",
    "from timeit import timeit\n",
    "\n",
    "config_list = [{\n",
    "    'op_types': ['Conv2d'],\n",
    "    'total_sparsity': 0.5\n",
    "},{\n",
    "    'exclude': True,\n",
    "    'op_names': [],\n",
    "    'op_types': []\n",
    "}]\n",
    "\n",
    "best_person_only_model = model.load_from_checkpoint(checkpoint_path=checkpointing.best_model_path)\n",
    "\n",
    "\n",
    "\n",
    "inference_time_unpruned = \n",
    "\n",
    "# Setting up callbacks\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "pruning_trainer = nni.trace(pl.Trainer)(\n",
    "    auto_scale_batch_size='binsearch',\n",
    "    accelerator=\"auto\",\n",
    "    devices=[0] if torch.cuda.is_available() else None,\n",
    "    accumulate_grad_batches=1,\n",
    "    logger=tensorboard,\n",
    "    log_every_n_steps=1,\n",
    "    fast_dev_run= True if not torch.cuda.is_available() else False,\n",
    "    callbacks=[\n",
    "    lr_monitor,  \n",
    "    ]\n",
    ")\n",
    "\n",
    "pruned_model = auto_prune(model=best_person_only_model, trainer=pruning_trainer, datamodule=data, config_list=config_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embedded-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0ac39a595945313e65040e96e7adf1cc8b7f37ac143411dbf2794ef8b4649db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
